{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")\n",
    "# %cd /content/drive/MyDrive/CS5284_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dgl==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"actors_overview.csv\", header=None)\n",
    "df = df[0].str.split(\" \", expand=True)\n",
    "\n",
    "train_path = []\n",
    "train_number = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    train_path.append(df.iloc[i, 0])\n",
    "    train_number.append(eval(df.iloc[i, 1]))\n",
    "  \n",
    "# Take a subset to test the model\n",
    "MAX_NUM = 100\n",
    "\n",
    "train_number = np.array(train_number)\n",
    "indicies = np.argsort(train_number)\n",
    "\n",
    "sub_train_path = []\n",
    "sub_train_number = []\n",
    "cnt = 0\n",
    "for index in indicies:\n",
    "    sub_train_path.append(train_path[index])\n",
    "    cnt += train_number[index]\n",
    "    if cnt >= MAX_NUM:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `networkx` To `dgl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "NUM_NODE = 100\n",
    "\n",
    "TYPES = [\"THREAD\", \"FILE\", \"REGISTRY\", \"FLOW\", \"USER_SESSION\", \"SERVICE\", \"PROCESS\", \"MODULE\", \"TASK\", \"SHELL\"]\n",
    "TYPE_MAP = {type: i for i, type in enumerate(TYPES)}\n",
    "\n",
    "ACTIONS = [\n",
    "    \"FILE_CREATE\", \"FILE_DELETE\", \"FILE_MODIFY\", \"FILE_READ\", \"FILE_RENAME\",\n",
    "    \"FILE_WRITE\", \"FLOW_MESSAGE\", \"FLOW_OPEN\", \"MODULE_LOAD\", \"PROCESS_CREATE\",\n",
    "    \"PROCESS_OPEN\", \"PROCESS_TERMINATE\", \"REGISTRY_ADD\", \"REGISTRY_EDIT\",\n",
    "    \"REGISTRY_REMOVE\", \"SERVICE_CREATE\", \"SHELL_COMMAND\", \"TASK_CREATE\",\n",
    "    \"TASK_DELETE\", \"TASK_MODIFY\", \"TASK_START\", \"THREAD_CREATE\", \"THREAD_REMOTE_CREATE\",\n",
    "    \"THREAD_TERMINATE\", \"USER_SESSION_GRANT\", \"USER_SESSION_INTERACTIVE\",\"USER_SESSION_LOGIN\",\n",
    "    \"USER_SESSION_LOGOUT\", \"USER_SESSION_REMOTE\", \"USER_SESSION_UNLOCK\"\n",
    "    ]\n",
    "ACTION_MAP = {action: i for i, action in enumerate(ACTIONS)}\n",
    "\n",
    "def nx_to_dgl(nx_graph):\n",
    "    dgl_graph = dgl.graph(([], []))\n",
    "\n",
    "    node_ids = list(nx_graph.nodes)\n",
    "    dgl_graph.add_nodes(len(node_ids))\n",
    "    nx_to_dgl_node_map = {node: i for i, node in enumerate(node_ids)}\n",
    "    if len(node_ids) != NUM_NODE:\n",
    "        raise ValueError(\"Number of nodes in graph is not 100\")\n",
    "    \n",
    "    id_set = set()\n",
    "    for node in node_ids:\n",
    "        node_data = nx_graph.nodes[node]\n",
    "        node_id = node_data.get(\"nodeID\", -1)\n",
    "        id_set.add(node_id)\n",
    "    \n",
    "    # Map nodeIDs to random integer in range of [0, 99], avoid overfitting to nodeIDs\n",
    "    id_set_len = len(id_set)\n",
    "    rand_ids = np.random.choice(100, id_set_len, replace=False)\n",
    "    rand_id_Map = {node_id: rand_id for node_id, rand_id in zip(id_set, rand_ids)}\n",
    "    \n",
    "    # Map node features\n",
    "    node_types = []\n",
    "    ids = []\n",
    "    \n",
    "    for node in node_ids:\n",
    "        node_data = nx_graph.nodes[node]\n",
    "        node_type = node_data.get(\"node_type\", \"\")\n",
    "        node_id = node_data.get(\"nodeID\", -1)\n",
    "        node_types.append(TYPE_MAP.get(node_type, -1))\n",
    "        ids.append(rand_id_Map.get(node_id, -1))\n",
    "    \n",
    "    dgl_graph.ndata[\"type\"] = torch.tensor(node_types, dtype=torch.float32)\n",
    "    dgl_graph.ndata[\"id\"] = torch.tensor(ids, dtype=torch.float32)\n",
    "    \n",
    "    # Map edge features\n",
    "    edges = list(nx_graph.edges(data=True))\n",
    "    src_nodes = []\n",
    "    dst_nodes = []\n",
    "    edge_actions = []\n",
    "    \n",
    "    for edge in edges:\n",
    "        src, dst, edge_data = edge\n",
    "        src_nodes.append(nx_to_dgl_node_map[src])\n",
    "        dst_nodes.append(nx_to_dgl_node_map[dst])\n",
    "        edge_actions.append(ACTION_MAP.get(edge_data.get(\"action\", \"\"), -1))\n",
    "    \n",
    "    dgl_graph.add_edges(src_nodes, dst_nodes)\n",
    "    dgl_graph.edata[\"action\"] = torch.tensor(edge_actions, dtype=torch.float32)\n",
    "    \n",
    "    return dgl_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "dir_path = \"data/train_graphs/\"\n",
    "if not os.path.exists(dir_path):\n",
    "    raise ValueError(\"The path does not exist\")\n",
    "\n",
    "NUM_GRAPH = None\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for chid_dirs in os.listdir(dir_path):\n",
    "    chid_dirs_path = os.path.join(dir_path, chid_dirs)\n",
    "    if not os.path.isdir(chid_dirs_path):\n",
    "        continue\n",
    "    for file in os.listdir(chid_dirs_path):\n",
    "        if file.endswith(\".gz\"):\n",
    "            file_path = os.path.join(chid_dirs_path, file)\n",
    "            with gzip.open(file_path, \"rb\") as f:\n",
    "                nx_graph = nx.read_gml(f)\n",
    "                dgl_graph = nx_to_dgl(nx_graph)\n",
    "                graph_list.append(dgl_graph)\n",
    "                \n",
    "NUM_GRAPH = len(graph_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Collate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def get_weighted_adjacency_matrix(graph):\n",
    "    adj_spr = graph.adjacency_matrix(scipy_fmt=\"coo\")\n",
    "    edge_freq = graph.edata[\"action\"]\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    adj = torch.zeros(num_nodes, num_nodes)\n",
    "    for i, j, freq in zip(adj_spr.row, adj_spr.col, edge_freq):\n",
    "        adj[i, j] = freq\n",
    "    adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "\n",
    "def batch_save(graph_list, path, index, batch_size=32):\n",
    "    if len(graph_list) < batch_size:\n",
    "        batch_size = len(graph_list)\n",
    "    node_type_list = []\n",
    "    node_id_list = []\n",
    "    edge_action_list = []\n",
    "    for graph in graph_list:\n",
    "        node_type_list.append(graph.ndata[\"type\"])\n",
    "        node_id_list.append(graph.ndata[\"id\"])\n",
    "        adj = get_weighted_adjacency_matrix(graph)\n",
    "        edge_action_list.append(adj)\n",
    "    h = torch.cat(node_type_list, dim=0).view(batch_size, 100).long()\n",
    "    pe = torch.cat(node_id_list, dim=0).view(batch_size, 100).long()\n",
    "    e = torch.cat(edge_action_list, dim=0).view(batch_size, 100, 100).long()\n",
    "    torch.save(h, path + \"h_\" + str(index) + \".pt\")\n",
    "    torch.save(pe, path + \"pe_\" + str(index) + \".pt\")\n",
    "    torch.save(e, path + \"e_\" + str(index) + \".pt\")\n",
    "    return h, pe, e\n",
    "\n",
    "\n",
    "def collate_fn(graph_list=None, batch_size=32, path=\"data/train/\", index=0, load=True):\n",
    "    if not load:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        num_batch = ceil(len(graph_list) / batch_size)\n",
    "        for i in range(num_batch):\n",
    "            end_batch_index = (i + 1) * batch_size\n",
    "            if end_batch_index > len(graph_list):\n",
    "                end_batch_index = len(graph_list)\n",
    "            batch_graph_list = graph_list[i * batch_size: end_batch_index]\n",
    "            save_path = path + \"batch_\" + str(i) + \"/\"\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            batch_save(batch_graph_list, save_path, i)\n",
    "            h = None; pe = None; e = None\n",
    "    else:\n",
    "        save_path = path + \"batch_\" + str(index) + \"/\"\n",
    "        if not os.path.exists(save_path):\n",
    "            raise ValueError(\"The path does not exist\")\n",
    "        h = torch.load(save_path + \"h_\" + str(index) + \".pt\", weights_only=True)\n",
    "        pe = torch.load(save_path + \"pe_\" + str(index) + \".pt\", weights_only=True)\n",
    "        e = torch.load(save_path + \"e_\" + str(index) + \".pt\", weights_only=True)\n",
    "    return h, pe, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = collate_fn(graph_list, batch_size=BATCH_SIZE, load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def sym_tensor(x):\n",
    "    x = x.permute(0, 3, 1, 2) # [bs, n, n, d]\n",
    "    triu = torch.triu(x,diagonal=1).transpose(3,2) # [bs, d, n, n]\n",
    "    mask = (triu.abs()>0).float()                  # [bs, d, n, n]\n",
    "    x =  x * (1 - mask ) + mask * triu             # [bs, d, n, n]\n",
    "    x = x.permute(0, 2, 3, 1) # [bs, n, n, d]\n",
    "    return x               # [bs, n, n, d]\n",
    "\n",
    "class Embed_G(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        self.Embed_h = nn.Embedding(params[\"num_type\"], d)\n",
    "        self.Embed_e = nn.Embedding(params[\"num_action\"], d)\n",
    "        self.Embed_pe = nn.Embedding(params[\"num_node\"], d)\n",
    "    \n",
    "    def forward(self, h, e, pe):\n",
    "        pe = self.Embed_pe(pe) # [bs, n, d]\n",
    "        h = self.Embed_h(h)\n",
    "        h = h + pe\n",
    "        e = self.Embed_e(e) # [bs, n, n, d]\n",
    "        e = e + pe.unsqueeze(1) + pe.unsqueeze(2)\n",
    "        e = sym_tensor(e)\n",
    "        return h, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General GT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Layer(nn.Module):\n",
    "    def __init__(self, d, d_head, drop):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "        self.Drop_Att = nn.Dropout(drop)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        Q = self.Q(h) # [bs, n, d_head]\n",
    "        K = self.K(h)\n",
    "        V = self.V(h)\n",
    "        Q = Q.unsqueeze(2)  # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1)  # [bs, 1, n, d_head]\n",
    "        E = self.E(e)       # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(h).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(h).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = E + Ni + Nj              # [bs, n, n, d_head]\n",
    "        Att = (Q * e * K).sum(dim=-1) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1)             # [bs, n, n]\n",
    "        Att = self.Drop_Att(Att)\n",
    "        h = Att @ V # [bs, n, d_head]\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class MAH_Layer(nn.Module):\n",
    "    def __init__(self, d, head_num, drop):\n",
    "        super().__init__()\n",
    "        d_head = d // head_num\n",
    "        self.heads = nn.ModuleList([Attention_Layer(d, d_head, drop) for _ in range(head_num)])\n",
    "        self.WO_h = nn.Linear(d, d)\n",
    "        self.WO_e = nn.Linear(d, d)\n",
    "        self.Drop_h = nn.Dropout(drop)\n",
    "        self.Drop_e = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        h_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            h_mha, e_mha = head(h, e)\n",
    "            h_MHA.append(h_mha)\n",
    "            e_MHA.append(e_mha)\n",
    "        h = self.Drop_h(self.WO_h(torch.cat(h_MHA, dim=2)))\n",
    "        e = self.Drop_e(self.WO_e(torch.cat(e_MHA, dim=3)))\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class GT_Layer(nn.Module):\n",
    "    def __init__(self, d, num_head, drop):\n",
    "        super().__init__()\n",
    "        self.Norm_h_1 = nn.LayerNorm(d)\n",
    "        self.Norm_e_1 = nn.LayerNorm(d)\n",
    "        self.MHA = MAH_Layer(d, num_head, drop)\n",
    "        self.Norm_h_2 = nn.LayerNorm(d)\n",
    "        self.Norm_e_2 = nn.LayerNorm(d)\n",
    "        self.MLP_h = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.MLP_e = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.Drop_h = nn.Dropout(drop)\n",
    "        self.Drop_e = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        h = self.Norm_h_1(h)\n",
    "        e = self.Norm_e_1(e)\n",
    "        h_MHA, e_MHA = self.MHA(h, e)\n",
    "        h = h + h_MHA\n",
    "        h = h + self.MLP_h(self.Norm_h_2(h))\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLP_e(self.Norm_e_2(e))\n",
    "        h = self.Drop_h(h)\n",
    "        e = self.Drop_e(e)\n",
    "        return h, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GT Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GT_Classifier(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        # Graph Embedding\n",
    "        self.Graph_Embedding = Embed_G(params)\n",
    "        # GT Layers\n",
    "        num_gt_layer = params[\"num_gt_layer\"]\n",
    "        num_head = params[\"num_head\"]\n",
    "        drop = params[\"drop\"]\n",
    "        self.GT_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_gt_layer)])\n",
    "        # Output Layer\n",
    "        self.Out = nn.Linear(d, 1)\n",
    "    \n",
    "    def forward(self, e, h, pe):\n",
    "        h, e = self.Graph_Embedding(h, e, pe)\n",
    "        for GT_Layer in self.GT_Layers:\n",
    "            h, e = GT_Layer(h, e)\n",
    "            e = sym_tensor(e)\n",
    "        graph_token = h.mean(dim=1) # [bs, d]\n",
    "        return self.Out(graph_token).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    "    \"d\": 32,\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_gt_layer\": 2,\n",
    "    \"num_head\": 4,\n",
    "    \"drop\": 0\n",
    "}\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "classifier = GT_Classifier(net_params)\n",
    "\n",
    "h, pe, e = collate_fn(index=0)\n",
    "out = classifier(e, h, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        \n",
    "        # Graph Embedding\n",
    "        self.num_node = params[\"num_node\"]\n",
    "        self.Embed_he = Embed_G(params)\n",
    "        self.Embed_pe = nn.Embedding(params[\"num_node\"], d)\n",
    "\n",
    "        # GT Layers\n",
    "        num_enc_layer = params[\"num_enc_layer\"]\n",
    "        num_dec_layer = params[\"num_dec_layer\"]\n",
    "        num_head = params[\"num_head\"]\n",
    "        drop = params[\"drop\"]\n",
    "        self.Enc_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_enc_layer)])\n",
    "        self.Dec_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_dec_layer)])\n",
    "\n",
    "        # Encoder\n",
    "        dz = params[\"dz\"]\n",
    "        self.LN_q_mu = nn.Linear(d, dz)\n",
    "        self.LN_q_logvar = nn.Linear(d, dz)\n",
    "\n",
    "        # Decoder\n",
    "        self.LN_p = nn.Linear(dz, d)\n",
    "\n",
    "        # Output Layer\n",
    "        self.Norm_Out_h = nn.LayerNorm(d)\n",
    "        self.Norm_Out_e = nn.LayerNorm(d)\n",
    "        self.LN_h = nn.Linear(d, params[\"num_type\"])\n",
    "        self.LN_e = nn.Linear(d, params[\"num_action\"])\n",
    "    \n",
    "    def forward(self, h, e, pe, num_node=None):\n",
    "        if num_node is None:\n",
    "            num_node = self.num_node\n",
    "\n",
    "        # Embedding\n",
    "        h, e = self.Embed_he(h, e, pe)\n",
    "        n = h.size(1)\n",
    "        pe = self.Embed_pe(pe)\n",
    "        # Encoder\n",
    "        for Enc_Layer in self.Enc_Layers:\n",
    "            h, e = Enc_Layer(h, e)\n",
    "            e = sym_tensor(e)\n",
    "        graph_token = h.mean(dim=1)\n",
    "        q_mu = self.LN_q_mu(graph_token)\n",
    "        q_logvar = self.LN_q_logvar(graph_token)\n",
    "        q_std = torch.exp(q_logvar / 2)\n",
    "        eps = torch.randn_like(q_std)\n",
    "        z = q_mu + eps * q_std # [bs, dz]\n",
    "        n = h.size(1)\n",
    "\n",
    "        # Decoder\n",
    "        z = self.LN_p(z) # [bs, d]\n",
    "        h = z.unsqueeze(1).repeat(1, n, 1) # [bs, n, d]\n",
    "        h = h + pe\n",
    "        e = z.unsqueeze(1).unsqueeze(1).repeat(1, n, n, 1) # [bs, n, n, d]\n",
    "        e = e + pe.unsqueeze(1) + pe.unsqueeze(2)\n",
    "        e = sym_tensor(e)\n",
    "        for Dec_Layer in self.Dec_Layers:\n",
    "            h, e = Dec_Layer(h, e)\n",
    "            e = sym_tensor(e)\n",
    "        h = self.Norm_Out_h(h)\n",
    "        e = self.Norm_Out_e(e)\n",
    "        h = self.LN_h(h)\n",
    "        e = self.LN_e(e)\n",
    "        return h, e, q_mu, q_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_enc_layer\": 4,\n",
    "    \"num_dec_layer\": 4,\n",
    "    \"num_head\": 8,\n",
    "    \"drop\": 0,\n",
    "    \"d\": 16 * 8,\n",
    "    \"dz\": 32\n",
    "}\n",
    "\n",
    "h, pe, e = collate_fn(index=0)\n",
    "vae = VAE(net_params)\n",
    "out_h, out_e, q_mu, q_logvar = vae(h, e, pe)\n",
    "\n",
    "\n",
    "print(\"h.shape:\", h.shape)\n",
    "print(\"pe.shape:\", pe.shape)\n",
    "print(\"e.shape:\", e.shape)\n",
    "print(\"out_h.shape:\", out_h.shape)\n",
    "print(\"out_e.shape:\", out_e.shape)\n",
    "print(\"q_mu.shape:\", q_mu.shape)\n",
    "print(\"q_logvar.shape:\", q_logvar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WARMUP = 2 * max(MAX_NUM, NUM_GRAPH // BATCH_SIZE)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "vae = VAE(net_params)\n",
    "vae.to(DEVICE)\n",
    "\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=init_lr)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/NUM_WARMUP, 1.0))\n",
    "scheduler_tracker = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=5)\n",
    "\n",
    "NUM_EPOCH = 1000\n",
    "num_warm_batch = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    running_loss = 0.0\n",
    "    num_batch = 0\n",
    "\n",
    "    vae.train()\n",
    "\n",
    "    MAX_BATCH = 312\n",
    "    for i in range(MAX_BATCH):\n",
    "        h, pe, e = collate_fn(graph_list, index=i)\n",
    "        h = h.to(DEVICE)\n",
    "        pe = pe.to(DEVICE)\n",
    "        e = e.to(DEVICE)\n",
    "        pred_h, pred_e, q_mu, q_logvar = vae(h, e, pe)\n",
    "        loss_data = torch.nn.CrossEntropyLoss()(pred_h.view(BATCH_SIZE * NUM_NODE, len(TYPES)), h.view(BATCH_SIZE * NUM_NODE)) \n",
    "        loss_data += torch.nn.CrossEntropyLoss()(pred_e.view(BATCH_SIZE * NUM_NODE * NUM_NODE, len(ACTIONS)), e.view(BATCH_SIZE * NUM_NODE * NUM_NODE))\n",
    "        loss_kl = -0.5 * torch.sum(1 + q_logvar - q_mu.pow(2) - q_logvar.exp())\n",
    "        loss = 2.5 * loss_data + loss_kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "        if num_warm_batch < NUM_WARMUP:\n",
    "            num_warm_batch += 1\n",
    "            scheduler_warmup.step()\n",
    "        num_warm_batch += 1\n",
    "\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batch += 1\n",
    "    \n",
    "    mean_loss = running_loss / num_batch\n",
    "    if num_warm_batch >= NUM_WARMUP:\n",
    "        scheduler_tracker.step(mean_loss)\n",
    "        num_warm_batch += 1\n",
    "    elapsed = (time.time() - start) / 60\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCH}  Loss: {mean_loss:.4f}  Elapsed Time: {elapsed:.2f} mins\")\n",
    "\n",
    "    if optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        # Graph Embedding\n",
    "        self.num_node = params[\"num_node\"]\n",
    "        self.num_t = params[\"num_t\"]\n",
    "        self.num_type = params[\"num_type\"]\n",
    "        self.num_action = params[\"num_action\"]\n",
    "        self.Embed_h = nn.Linear(self.num_type, d)\n",
    "        self.Embed_e = nn.Linear(self.num_action, d)\n",
    "        self.pe_h = nn.Embedding(self.num_node, d)\n",
    "        # Time Embedding\n",
    "        self.pe_t = nn.Sequential(nn.Embedding(self.num_t, d), nn.ReLU(), nn.Linear(d, d))\n",
    "        # GT Layers\n",
    "        num_gt_layer = params[\"num_gt_layer\"]\n",
    "        num_head = params[\"num_head\"]\n",
    "        drop = params[\"drop\"]\n",
    "        self.GT_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_gt_layer)])\n",
    "        # Output Layer\n",
    "        self.LN_h = nn.Linear(d, self.num_type)\n",
    "        self.LN_e = nn.Linear(d, self.num_action)\n",
    "    \n",
    "    def forward(self, h, e, pe_h, sample_t):\n",
    "        # Embedding for Graph\n",
    "        pe_h = self.pe_h(pe_h) # [bs, n, d]\n",
    "        h_t = self.Embed_h(h)\n",
    "        h_t = h_t + pe_h\n",
    "        e_t = self.Embed_e(e) # [bs, n, n, d]\n",
    "        e_t = e_t + pe_h.unsqueeze(1)\n",
    "        e_t = sym_tensor(e_t)\n",
    "        # Embedding for Time\n",
    "        pe_t = self.pe_t(sample_t) # [bs, d]\n",
    "        # GT Layers\n",
    "        for GT_Layer in self.GT_Layers:\n",
    "            h_t = h_t + pe_t.unsqueeze(1) # [bs, n, d]\n",
    "            e_t = e_t + pe_t.unsqueeze(1).unsqueeze(2) # [bs, n, n, d]\n",
    "            h_t, e_t = GT_Layer(h_t, e_t)\n",
    "            e_t = sym_tensor(e_t)\n",
    "        # Output Layer\n",
    "        h_t_minus_one = self.LN_h(h_t)\n",
    "        e_t_minus_one = self.LN_e(e_t)\n",
    "        return h_t_minus_one, e_t_minus_one\n",
    "    \n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, num_t, beta_1, beta_t, params):\n",
    "        super().__init__()\n",
    "        self.device = params[\"device\"]\n",
    "        self.num_type = params[\"num_type\"]\n",
    "        self.num_action = params[\"num_action\"]\n",
    "        self.UNet = UNet(params)\n",
    "        self.num_t = num_t\n",
    "        self.alpha_t = 1.0 - torch.linspace(beta_1, beta_t, num_t).to(self.device)\n",
    "        self.alpha_bar_t = torch.cumprod(self.alpha_t, dim=0)\n",
    "    \n",
    "    def forward(self, h_0, e_0, sample_t, noise_h0, noise_e0):\n",
    "        h0 = torch.nn.functional.one_hot(h_0, self.num_type).float()\n",
    "        e0 = torch.nn.functional.one_hot(e_0, self.num_action).float()\n",
    "        bs = len(sample_t)\n",
    "        sqrt_alpha_bar_t = torch.sqrt(self.alpha_bar_t[sample_t])\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - self.alpha_bar_t[sample_t])\n",
    "        h_t = sqrt_alpha_bar_t.view(bs, 1, 1) * h0 + sqrt_one_minus_alpha_bar_t.view(bs, 1, 1) * noise_h0\n",
    "        e_t = sqrt_alpha_bar_t.view(bs, 1, 1, 1) * e0 + sqrt_one_minus_alpha_bar_t.view(bs, 1, 1, 1) * noise_e0\n",
    "        return h_t, e_t\n",
    "    \n",
    "    def backward(self, h_t, e_t, pe_h, sample_t):\n",
    "        noise_pred_h_t, noise_pred_e_t = self.UNet(h_t, e_t, pe_h, sample_t)\n",
    "        return noise_pred_h_t, noise_pred_e_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "beta_1 = 0.0001\n",
    "beta_t = 0.1\n",
    "num_t = 200\n",
    "\n",
    "net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_gt_layer\": 6,\n",
    "    \"num_head\": 4,\n",
    "    \"d\": 32 * 4,\n",
    "    \"num_t\": num_t,\n",
    "    \"drop\": 0,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "num_warmup = 2 * max(NUM_NODE, NUM_GRAPH // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def train_ddpm(num_t, beta_1, beta_t, net_params, load_save=True, model_path=\"model/ddpm/\"):\n",
    "    torch.random.manual_seed(0)\n",
    "    ddpm = DDPM(num_t, beta_1, beta_t, net_params).to(device)\n",
    "    if load_save:\n",
    "        if os.path.exists(model_path + \"ddpm.pt\"):\n",
    "            ddpm.load_state_dict(torch.load(model_path + \"ddpm.pt\", weights_only=True))\n",
    "        \n",
    "\n",
    "    init_lr = 0.001\n",
    "    optimizer = torch.optim.AdamW(ddpm.parameters(), lr=init_lr)\n",
    "    scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0))\n",
    "    scheduler_tracker = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1)\n",
    "\n",
    "    num_epoch = 1000\n",
    "    num_warmup_batch = 0\n",
    "    MAX_BATCH = 312\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        num_batch = 0\n",
    "\n",
    "        ddpm.train()\n",
    "\n",
    "        for i in range(MAX_BATCH):\n",
    "            h, pe, e = collate_fn(index=i)\n",
    "            h = h.to(device)\n",
    "            pe = pe.to(device)\n",
    "            e = e.to(device)\n",
    "            batch_sample_t = torch.randint(0, num_t, (BATCH_SIZE,)).long().to(device)\n",
    "            batch_noise_h_t = torch.randn(BATCH_SIZE, NUM_NODE, len(TYPES)).to(device)\n",
    "            batch_noise_e_t = torch.randn(BATCH_SIZE, NUM_NODE, NUM_NODE, len(ACTIONS)).to(device)\n",
    "            batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
    "\n",
    "            h_t, e_t = ddpm(h, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t)\n",
    "            noise_pred_h_t, noise_pred_e_t = ddpm.backward(h_t, e_t, pe, batch_sample_t)\n",
    "\n",
    "            loss = torch.nn.MSELoss()(noise_pred_h_t, batch_noise_h_t) + torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if num_batch < NUM_WARMUP:\n",
    "                scheduler_warmup.step()\n",
    "            num_batch += 1\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "            num_batch += 1\n",
    "\n",
    "            del h, pe, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t, h_t, e_t, noise_pred_h_t, noise_pred_e_t\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        mean_loss = running_loss / num_batch\n",
    "        if num_warmup_batch >= num_warmup:\n",
    "            scheduler_tracker.step(mean_loss)\n",
    "        elapsed = (time.time() - start) / 60\n",
    "        print(f\"Epoch  {epoch+1}/{num_epoch}  Loss: {mean_loss:.4f}  Time: {elapsed:.2f}\")\n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            torch.save(ddpm.state_dict(), model_path + \"ddpm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddpm(num_t, beta_1, beta_t, net_params, load_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
