{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Based on Generation Models\n",
    "\n",
    "In this section, we utilize the graph-based generation models to detect the abnormal/malicious data in the log dataset. Generation models are able to learn the underlying data distribution of the normal data. When the model is trained on the normal dataset, if a normal data sample is input and forwared through the model, the reconstruction error after the backward propagation should keeps low. However, if the input is an abnormal data sample, the reconstruction error will be relatively much higher. Therefore, through analyzing the reconstruction error, we are able to detect the abnormal data points feeded into the model. We conducted the data analysis leveraging the [Denoising Diffusion Probabilistic Model][1] (DDPM), with the [Variational Autoencoder][2] (VAE) as the comparison benchmark. Both models are integrated with the [Graph Transformer][3] model to capture the graph information.\n",
    "\n",
    "The training was conducted on 56,576 normal log graphs. Due to the data loeading and training process of generation models are time-consuming, we used sperate Python files to reconstruct the graph data, collate the data batches, train the models, and saved the trained model parameters. The specific code files and example tensor format are available in the GitHub repository [here](https://github.com/Klasnov/graph_abnormal_detect). In this report section, we will show the code snippets ***without*** executed outputs. The code content in this section is adapted from the tutorial code files, with modifications to our specific dataset application, like the explicit positional encoding of the graph structure based on the log sequence.\n",
    "\n",
    "[1]: https://arxiv.org/pdf/2006.11239\n",
    "[2]: https://arxiv.org/pdf/1312.6114\n",
    "[3]: https://arxiv.org/pdf/2012.09699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Graph Reconstruction with DGL Library\n",
    "\n",
    "We first load the networkx graph files after the previous data preprocessing. The graph data is then converted to Tensor format with DGL library, as DGL can help to easily extract and manipulate the node and edge features in the graph data.\n",
    "\n",
    "In the networkx graph, the node features we are going to use are the node type in the computational system and the node log ID. Since some ID values are very frequent appearing in the log data, for each graph with specific size of 100 nodes, we reindex the node local ID, as the node positional encoding later, by random values in the range of 0 to 99. This may avoid the model to overfit on the specific global node ID values. The edge features are the operation type between the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "NUM_NODE = 100\n",
    "\n",
    "TYPES = [\"THREAD\", \"FILE\", \"REGISTRY\", \"FLOW\", \"USER_SESSION\", \"SERVICE\", \"PROCESS\", \"MODULE\", \"TASK\", \"SHELL\"]\n",
    "TYPE_MAP = {type: i for i, type in enumerate(TYPES)}\n",
    "\n",
    "ACTIONS = [\n",
    "    \"FILE_CREATE\", \"FILE_DELETE\", \"FILE_MODIFY\", \"FILE_READ\", \"FILE_RENAME\",\n",
    "    \"FILE_WRITE\", \"FLOW_MESSAGE\", \"FLOW_OPEN\", \"MODULE_LOAD\", \"PROCESS_CREATE\",\n",
    "    \"PROCESS_OPEN\", \"PROCESS_TERMINATE\", \"REGISTRY_ADD\", \"REGISTRY_EDIT\",\n",
    "    \"REGISTRY_REMOVE\", \"SERVICE_CREATE\", \"SHELL_COMMAND\", \"TASK_CREATE\",\n",
    "    \"TASK_DELETE\", \"TASK_MODIFY\", \"TASK_START\", \"THREAD_CREATE\", \"THREAD_REMOTE_CREATE\",\n",
    "    \"THREAD_TERMINATE\", \"USER_SESSION_GRANT\", \"USER_SESSION_INTERACTIVE\",\"USER_SESSION_LOGIN\",\n",
    "    \"USER_SESSION_LOGOUT\", \"USER_SESSION_REMOTE\", \"USER_SESSION_UNLOCK\"\n",
    "    ]\n",
    "ACTION_MAP = {action: i for i, action in enumerate(ACTIONS)}\n",
    "\n",
    "def nx_to_dgl(nx_graph):\n",
    "    dgl_graph = dgl.graph(([], []))\n",
    "\n",
    "    node_ids = list(nx_graph.nodes)\n",
    "    dgl_graph.add_nodes(len(node_ids))\n",
    "    nx_to_dgl_node_map = {node: i for i, node in enumerate(node_ids)}\n",
    "    if len(node_ids) != NUM_NODE:\n",
    "        raise ValueError(\"Number of nodes in graph is not 100\")\n",
    "    \n",
    "    id_set = set()\n",
    "    for node in node_ids:\n",
    "        node_data = nx_graph.nodes[node]\n",
    "        node_id = node_data.get(\"nodeID\", -1)\n",
    "        id_set.add(node_id)\n",
    "    \n",
    "    # Map nodeIDs to random integer in range of [0, 99], avoid overfitting to nodeIDs\n",
    "    id_set_len = len(id_set)\n",
    "    rand_ids = np.random.choice(100, id_set_len, replace=False)\n",
    "    rand_id_Map = {node_id: rand_id for node_id, rand_id in zip(id_set, rand_ids)}\n",
    "    \n",
    "    # Map node features\n",
    "    node_types = []\n",
    "    ids = []\n",
    "    \n",
    "    for node in node_ids:\n",
    "        node_data = nx_graph.nodes[node]\n",
    "        node_type = node_data.get(\"node_type\", \"\")\n",
    "        node_id = node_data.get(\"nodeID\", -1)\n",
    "        node_types.append(TYPE_MAP.get(node_type, -1))\n",
    "        ids.append(rand_id_Map.get(node_id, -1))\n",
    "    \n",
    "    dgl_graph.ndata[\"type\"] = torch.tensor(node_types, dtype=torch.float32)\n",
    "    dgl_graph.ndata[\"id\"] = torch.tensor(ids, dtype=torch.float32)\n",
    "    \n",
    "    # Map edge features\n",
    "    edges = list(nx_graph.edges(data=True))\n",
    "    src_nodes = []\n",
    "    dst_nodes = []\n",
    "    edge_actions = []\n",
    "    \n",
    "    for edge in edges:\n",
    "        src, dst, edge_data = edge\n",
    "        src_nodes.append(nx_to_dgl_node_map[src])\n",
    "        dst_nodes.append(nx_to_dgl_node_map[dst])\n",
    "        edge_actions.append(ACTION_MAP.get(edge_data.get(\"action\", \"\"), -1))\n",
    "    \n",
    "    dgl_graph.add_edges(src_nodes, dst_nodes)\n",
    "    dgl_graph.edata[\"action\"] = torch.tensor(edge_actions, dtype=torch.float32)\n",
    "    \n",
    "    return dgl_graph\n",
    "\n",
    "\n",
    "\n",
    "dir_path = \"data/train_graphs/\"\n",
    "if not os.path.exists(dir_path):\n",
    "    raise ValueError(\"The path does not exist\")\n",
    "\n",
    "train_graph_list = []\n",
    "for chid_dirs in os.listdir(dir_path):\n",
    "    chid_dirs_path = os.path.join(dir_path, chid_dirs)\n",
    "    if not os.path.isdir(chid_dirs_path):\n",
    "        continue\n",
    "    for file in os.listdir(chid_dirs_path):\n",
    "        if file.endswith(\".gz\"):\n",
    "            file_path = os.path.join(chid_dirs_path, file)\n",
    "            with gzip.open(file_path, \"rb\") as f:\n",
    "                nx_graph = nx.read_gml(f)\n",
    "                dgl_graph = nx_to_dgl(nx_graph)\n",
    "                train_graph_list.append(dgl_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batching and Storage in Tensor Format\n",
    "\n",
    "To make the training process more efficient, we collate the graph data into batches with batch size of 32. The batched graph data is saved in the PyTorch tensor format, which can be directly loaded in the training process.\n",
    "\n",
    "In this process, the edge features are converted from the coordinate sparse matrix format (COO) into the dense weighted adjacency matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def get_weighted_adjacency_matrix(graph):\n",
    "    adj_spr = graph.adjacency_matrix(scipy_fmt=\"coo\")\n",
    "    edge_freq = graph.edata[\"action\"]\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    adj = torch.zeros(num_nodes, num_nodes)\n",
    "    for i, j, freq in zip(adj_spr.row, adj_spr.col, edge_freq):\n",
    "        adj[i, j] = freq\n",
    "    adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "\n",
    "def batch_save(graph_list, path, index, batch_size=32):\n",
    "    if len(graph_list) < batch_size:\n",
    "        batch_size = len(graph_list)\n",
    "    node_type_list = []\n",
    "    node_id_list = []\n",
    "    edge_action_list = []\n",
    "    for graph in graph_list:\n",
    "        node_type_list.append(graph.ndata[\"type\"])\n",
    "        node_id_list.append(graph.ndata[\"id\"])\n",
    "        adj = get_weighted_adjacency_matrix(graph)\n",
    "        edge_action_list.append(adj)\n",
    "    h = torch.cat(node_type_list, dim=0).view(batch_size, 100).long()\n",
    "    pe = torch.cat(node_id_list, dim=0).view(batch_size, 100).long()\n",
    "    e = torch.cat(edge_action_list, dim=0).view(batch_size, 100, 100).long()\n",
    "    torch.save(h, path + \"h_\" + str(index) + \".pt\")\n",
    "    torch.save(pe, path + \"pe_\" + str(index) + \".pt\")\n",
    "    torch.save(e, path + \"e_\" + str(index) + \".pt\")\n",
    "    return h, pe, e\n",
    "\n",
    "\n",
    "def batch_graph(graph_list=None, batch_size=32, path=\"data/train/\", index=0, load=True):\n",
    "    if not load:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        num_batch = ceil(len(graph_list) / batch_size)\n",
    "        for i in range(num_batch):\n",
    "            end_batch_index = (i + 1) * batch_size\n",
    "            if end_batch_index > len(graph_list):\n",
    "                end_batch_index = len(graph_list)\n",
    "            batch_graph_list = graph_list[i * batch_size: end_batch_index]\n",
    "            save_path = path + \"batch_\" + str(i) + \"/\"\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            batch_save(batch_graph_list, save_path, i)\n",
    "            h = None; pe = None; e = None\n",
    "    else:\n",
    "        save_path = path + \"batch_\" + str(index) + \"/\"\n",
    "        if not os.path.exists(save_path):\n",
    "            raise ValueError(\"The path does not exist\")\n",
    "        h = torch.load(save_path + \"h_\" + str(index) + \".pt\", weights_only=True)\n",
    "        pe = torch.load(save_path + \"pe_\" + str(index) + \".pt\", weights_only=True)\n",
    "        e = torch.load(save_path + \"e_\" + str(index) + \".pt\", weights_only=True)\n",
    "    return h, pe, e\n",
    "\n",
    "\n",
    "_, _, _ = batch_graph(train_graph_list, batch_size=BATCH_SIZE, load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script for DDPM Model\n",
    "\n",
    "The training script for the Denoising Diffusion Probabilistic Model (DDPM) is shown below. The model is trained with the batched graph data, and the trained model parameters are saved for the later reconstruction and analysis process.\n",
    "\n",
    "The batch size was set to 32, with the number of batches was 1768. Total 1000 epochs were set for the training process. The initial learning rate was set to 0.0003. We adapt the Adam optimizer for the model training as the tutorial code. Warmup learning rate scheduler and reduce learning rate on plateau scheduler are also used in the training process. The loss function used in the training process is the mean squared error (MSE) loss to identify whether the model correctly learned the denoising process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "NUM_NODE = 100\n",
    "TYPES = [\"THREAD\", \"FILE\", \"REGISTRY\", \"FLOW\", \"USER_SESSION\", \"SERVICE\", \"PROCESS\", \"MODULE\", \"TASK\", \"SHELL\"]\n",
    "ACTIONS = [\n",
    "    \"FILE_CREATE\", \"FILE_DELETE\", \"FILE_MODIFY\", \"FILE_READ\", \"FILE_RENAME\", \"FILE_WRITE\", \"FLOW_MESSAGE\",\n",
    "    \"FLOW_OPEN\", \"MODULE_LOAD\", \"PROCESS_CREATE\", \"PROCESS_OPEN\", \"PROCESS_TERMINATE\", \"REGISTRY_ADD\",\n",
    "    \"REGISTRY_EDIT\", \"REGISTRY_REMOVE\", \"SERVICE_CREATE\", \"SHELL_COMMAND\", \"TASK_CREATE\", \"TASK_DELETE\",\n",
    "    \"TASK_MODIFY\", \"TASK_START\", \"THREAD_CREATE\", \"THREAD_REMOTE_CREATE\", \"THREAD_TERMINATE\", \"USER_SESSION_GRANT\",\n",
    "    \"USER_SESSION_INTERACTIVE\",\"USER_SESSION_LOGIN\", \"USER_SESSION_LOGOUT\", \"USER_SESSION_REMOTE\", \"USER_SESSION_UNLOCK\"\n",
    "]\n",
    "NUM_BATCH = 1768\n",
    "NUM_GRAPH = NUM_BATCH * BATCH_SIZE\n",
    "NUM_WARMUP = 2 * max(NUM_NODE, NUM_GRAPH // BATCH_SIZE)\n",
    "\n",
    "EPOCH_NUM = 1000\n",
    "INIT_LR = 0.0003\n",
    "\n",
    "\n",
    "def get_weighted_adjacency_matrix(graph):\n",
    "    adj_spr = graph.adjacency_matrix(scipy_fmt=\"coo\")\n",
    "    edge_freq = graph.edata[\"action\"]\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    adj = torch.zeros(num_nodes, num_nodes)\n",
    "    for i, j, freq in zip(adj_spr.row, adj_spr.col, edge_freq):\n",
    "        adj[i, j] = freq\n",
    "    adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "\n",
    "def get_batch(path=\"data/train/\", index=0):\n",
    "    save_path = path + \"batch_\" + str(index) + \"/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        raise ValueError(\"The path does not exist\")\n",
    "    h = torch.load(save_path + \"h_\" + str(index) + \".pt\", weights_only=True)\n",
    "    pe = torch.load(save_path + \"pe_\" + str(index) + \".pt\", weights_only=True)\n",
    "    e = torch.load(save_path + \"e_\" + str(index) + \".pt\", weights_only=True)\n",
    "    return h, pe, e\n",
    "\n",
    "\n",
    "def sym_tensor(x):\n",
    "    x = x.permute(0, 3, 1, 2) # [bs, n, n, d]\n",
    "    triu = torch.triu(x,diagonal=1).transpose(3,2) # [bs, d, n, n]\n",
    "    mask = (triu.abs()>0).float()                  # [bs, d, n, n]\n",
    "    x =  x * (1 - mask ) + mask * triu             # [bs, d, n, n]\n",
    "    x = x.permute(0, 2, 3, 1) # [bs, n, n, d]\n",
    "    return x               # [bs, n, n, d]\n",
    "\n",
    "class Embed_G(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        self.Embed_h = nn.Embedding(params[\"num_type\"], d)\n",
    "        self.Embed_e = nn.Embedding(params[\"num_action\"], d)\n",
    "        self.Embed_pe = nn.Embedding(params[\"num_node\"], d)\n",
    "    \n",
    "    def forward(self, h, e, pe):\n",
    "        pe = self.Embed_pe(pe) # [bs, n, d]\n",
    "        h = self.Embed_h(h)\n",
    "        h = h + pe\n",
    "        e = self.Embed_e(e) # [bs, n, n, d]\n",
    "        e = e + pe.unsqueeze(1) + pe.unsqueeze(2)\n",
    "        e = sym_tensor(e)\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class Attention_Layer(nn.Module):\n",
    "    def __init__(self, d, d_head, drop):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "        self.Drop_Att = nn.Dropout(drop)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        Q = self.Q(h) # [bs, n, d_head]\n",
    "        K = self.K(h)\n",
    "        V = self.V(h)\n",
    "        Q = Q.unsqueeze(2)  # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1)  # [bs, 1, n, d_head]\n",
    "        E = self.E(e)       # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(h).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(h).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = E + Ni + Nj              # [bs, n, n, d_head]\n",
    "        Att = (Q * e * K).sum(dim=-1) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1)             # [bs, n, n]\n",
    "        Att = self.Drop_Att(Att)\n",
    "        h = Att @ V # [bs, n, d_head]\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class MAH_Layer(nn.Module):\n",
    "    def __init__(self, d, head_num, drop):\n",
    "        super().__init__()\n",
    "        d_head = d // head_num\n",
    "        self.heads = nn.ModuleList([Attention_Layer(d, d_head, drop) for _ in range(head_num)])\n",
    "        self.WO_h = nn.Linear(d, d)\n",
    "        self.WO_e = nn.Linear(d, d)\n",
    "        self.Drop_h = nn.Dropout(drop)\n",
    "        self.Drop_e = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        h_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            h_mha, e_mha = head(h, e)\n",
    "            h_MHA.append(h_mha)\n",
    "            e_MHA.append(e_mha)\n",
    "        h = self.Drop_h(self.WO_h(torch.cat(h_MHA, dim=2)))\n",
    "        e = self.Drop_e(self.WO_e(torch.cat(e_MHA, dim=3)))\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class GT_Layer(nn.Module):\n",
    "    def __init__(self, d, num_head, drop):\n",
    "        super().__init__()\n",
    "        self.Norm_h_1 = nn.LayerNorm(d)\n",
    "        self.Norm_e_1 = nn.LayerNorm(d)\n",
    "        self.MHA = MAH_Layer(d, num_head, drop)\n",
    "        self.Norm_h_2 = nn.LayerNorm(d)\n",
    "        self.Norm_e_2 = nn.LayerNorm(d)\n",
    "        self.MLP_h = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.MLP_e = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.Drop_h = nn.Dropout(drop)\n",
    "        self.Drop_e = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        h = self.Norm_h_1(h)\n",
    "        e = self.Norm_e_1(e)\n",
    "        h_MHA, e_MHA = self.MHA(h, e)\n",
    "        h = h + h_MHA\n",
    "        h = h + self.MLP_h(self.Norm_h_2(h))\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLP_e(self.Norm_e_2(e))\n",
    "        h = self.Drop_h(h)\n",
    "        e = self.Drop_e(e)\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        # Graph Embedding\n",
    "        self.num_node = params[\"num_node\"]\n",
    "        self.num_t = params[\"num_t\"]\n",
    "        self.num_type = params[\"num_type\"]\n",
    "        self.num_action = params[\"num_action\"]\n",
    "        self.Embed_h = nn.Linear(self.num_type, d)\n",
    "        self.Embed_e = nn.Linear(self.num_action, d)\n",
    "        self.pe_h = nn.Embedding(self.num_node, d)\n",
    "        # Time Embedding\n",
    "        self.pe_t = nn.Sequential(nn.Embedding(self.num_t, d), nn.ReLU(), nn.Linear(d, d))\n",
    "        # GT Layers\n",
    "        num_gt_layer = params[\"num_gt_layer\"]\n",
    "        num_head = params[\"num_head\"]\n",
    "        drop = params[\"drop\"]\n",
    "        self.GT_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_gt_layer)])\n",
    "        # Output Layer\n",
    "        self.LN_h = nn.Linear(d, self.num_type)\n",
    "        self.LN_e = nn.Linear(d, self.num_action)\n",
    "    \n",
    "    def forward(self, h, e, pe_h, sample_t):\n",
    "        # Embedding for Graph\n",
    "        pe_h = self.pe_h(pe_h) # [bs, n, d]\n",
    "        h_t = self.Embed_h(h)\n",
    "        h_t = h_t + pe_h\n",
    "        e_t = self.Embed_e(e) # [bs, n, n, d]\n",
    "        e_t = e_t + pe_h.unsqueeze(1)\n",
    "        e_t = sym_tensor(e_t)\n",
    "        # Embedding for Time\n",
    "        pe_t = self.pe_t(sample_t) # [bs, d]\n",
    "        # GT Layers\n",
    "        for GT_Layer in self.GT_Layers:\n",
    "            h_t = h_t + pe_t.unsqueeze(1) # [bs, n, d]\n",
    "            e_t = e_t + pe_t.unsqueeze(1).unsqueeze(2) # [bs, n, n, d]\n",
    "            h_t, e_t = GT_Layer(h_t, e_t)\n",
    "            e_t = sym_tensor(e_t)\n",
    "        # Output Layer\n",
    "        h_t_minus_one = self.LN_h(h_t)\n",
    "        e_t_minus_one = self.LN_e(e_t)\n",
    "        return h_t_minus_one, e_t_minus_one\n",
    "    \n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, num_t, beta_1, beta_t, params):\n",
    "        super().__init__()\n",
    "        self.device = params[\"device\"]\n",
    "        self.num_type = params[\"num_type\"]\n",
    "        self.num_action = params[\"num_action\"]\n",
    "        self.UNet = UNet(params)\n",
    "        self.num_t = num_t\n",
    "        self.alpha_t = 1.0 - torch.linspace(beta_1, beta_t, num_t).to(self.device)\n",
    "        self.alpha_bar_t = torch.cumprod(self.alpha_t, dim=0)\n",
    "    \n",
    "    def forward(self, h_0, e_0, sample_t, noise_h0, noise_e0):\n",
    "        h0 = torch.nn.functional.one_hot(h_0, self.num_type).float()\n",
    "        e0 = torch.nn.functional.one_hot(e_0, self.num_action).float()\n",
    "        bs = len(sample_t)\n",
    "        sqrt_alpha_bar_t = torch.sqrt(self.alpha_bar_t[sample_t])\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - self.alpha_bar_t[sample_t])\n",
    "        h_t = sqrt_alpha_bar_t.view(bs, 1, 1) * h0 + sqrt_one_minus_alpha_bar_t.view(bs, 1, 1) * noise_h0\n",
    "        e_t = sqrt_alpha_bar_t.view(bs, 1, 1, 1) * e0 + sqrt_one_minus_alpha_bar_t.view(bs, 1, 1, 1) * noise_e0\n",
    "        return h_t, e_t\n",
    "    \n",
    "    def backward(self, h_t, e_t, pe_h, sample_t):\n",
    "        noise_pred_h_t, noise_pred_e_t = self.UNet(h_t, e_t, pe_h, sample_t)\n",
    "        return noise_pred_h_t, noise_pred_e_t\n",
    "    \n",
    "\n",
    "def train_ddpm(num_t, beta_1, beta_t, net_params, load_save=True, model_path=\"model/\"):\n",
    "    torch.random.manual_seed(0)\n",
    "    ddpm = DDPM(num_t, beta_1, beta_t, net_params).to(DEVICE)\n",
    "    if load_save:\n",
    "        if os.path.exists(model_path + \"ddpm.pt\"):\n",
    "            ddpm.load_state_dict(torch.load(model_path + \"ddpm.pt\", weights_only=True))\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    init_lr = INIT_LR\n",
    "    optimizer = torch.optim.AdamW(ddpm.parameters(), lr=init_lr)\n",
    "    scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/NUM_WARMUP, 1.0))\n",
    "    scheduler_tracker = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1)\n",
    "\n",
    "    num_epoch = EPOCH_NUM\n",
    "    num_warmup_batch = 0\n",
    "\n",
    "    train_loss_drop_patience = 5\n",
    "    loss_dropping = True\n",
    "    train_loss_drop_cnt = 0\n",
    "    previous_best_loss = float(\"inf\")\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        num_batch = 0\n",
    "\n",
    "        ddpm.train()\n",
    "\n",
    "        for i in range(NUM_BATCH):\n",
    "            h, pe, e = get_batch(index=i)\n",
    "            h = h.to(DEVICE)\n",
    "            pe = pe.to(DEVICE)\n",
    "            e = e.to(DEVICE)\n",
    "            batch_sample_t = torch.randint(0, num_t, (BATCH_SIZE,)).long().to(DEVICE)\n",
    "            batch_noise_h_t = torch.randn(BATCH_SIZE, NUM_NODE, len(TYPES)).to(DEVICE)\n",
    "            batch_noise_e_t = torch.randn(BATCH_SIZE, NUM_NODE, NUM_NODE, len(ACTIONS)).to(DEVICE)\n",
    "            batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
    "\n",
    "            h_t, e_t = ddpm(h, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t)\n",
    "            noise_pred_h_t, noise_pred_e_t = ddpm.backward(h_t, e_t, pe, batch_sample_t)\n",
    "\n",
    "            loss = torch.nn.MSELoss()(noise_pred_h_t, batch_noise_h_t) + torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if num_batch < NUM_WARMUP:\n",
    "                scheduler_warmup.step()\n",
    "            num_batch += 1\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "            num_batch += 1\n",
    "\n",
    "            del h, pe, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t, h_t, e_t, noise_pred_h_t, noise_pred_e_t\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        mean_loss = running_loss / num_batch\n",
    "        if num_warmup_batch >= NUM_WARMUP:\n",
    "            scheduler_tracker.step(mean_loss)\n",
    "        elapsed = (time.time() - start) / 60\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch}  Loss: {mean_loss:.6f}  lr: {optimizer.param_groups[0]['lr']:.6f}  Time: {elapsed:.2f} mins\")\n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "            print(\"Early stopping since lr < 1e-5\")\n",
    "            break\n",
    "\n",
    "        if mean_loss < previous_best_loss:\n",
    "            previous_best_loss = mean_loss\n",
    "            train_loss_drop_cnt = 0\n",
    "            loss_dropping = True\n",
    "        else:\n",
    "            train_loss_drop_cnt += 1\n",
    "            loss_dropping = False\n",
    "            if train_loss_drop_cnt >= train_loss_drop_patience:\n",
    "                print(\"Early stopping since loss is not dropping\")\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 and loss_dropping:\n",
    "            torch.save(ddpm.state_dict(), model_path + \"ddpm.pt\")\n",
    "    \n",
    "    torch.save(ddpm.state_dict(), model_path + \"ddpm.pt\")\n",
    "    print(f\"Finished the training of DDPM, with the best loss {previous_best_loss:.6f}, and the total time {elapsed:.2f} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed hyperparameter setting of the DDPM model is shown in the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = 0.0001\n",
    "beta_t = 0.1\n",
    "num_t = 200\n",
    "\n",
    "ddpm_net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_gt_layer\": 6,\n",
    "    \"num_head\": 4,\n",
    "    \"d\": 32 * 4,\n",
    "    \"num_t\": num_t,\n",
    "    \"drop\": 0,\n",
    "    \"device\": DEVICE\n",
    "}\n",
    "\n",
    "train_ddpm(num_t, beta_1, beta_t, ddpm_net_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script for VAE Model\n",
    "\n",
    "The training script for the Variational Autoencoder (VAE) model is shown below. The model is trained with the batched graph data, and the trained model parameters are saved for the later reconstruction and analysis process.\n",
    "\n",
    "The batch size was set to 32, with the number of batches was 1768. Total 1000 epochs were set for the training process. The initial learning rate was set to 0.0003. We adapt the Adam optimizer for the model training as the tutorial code. Warmup learning rate scheduler and reduce learning rate on plateau scheduler are also used in the training process. The loss function used in the training process is the cross entropy loss to identify whether the decoded data is correctly reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "NUM_NODE = 100\n",
    "TYPES = [\"THREAD\", \"FILE\", \"REGISTRY\", \"FLOW\", \"USER_SESSION\", \"SERVICE\", \"PROCESS\", \"MODULE\", \"TASK\", \"SHELL\"]\n",
    "ACTIONS = [\n",
    "    \"FILE_CREATE\", \"FILE_DELETE\", \"FILE_MODIFY\", \"FILE_READ\", \"FILE_RENAME\", \"FILE_WRITE\", \"FLOW_MESSAGE\",\n",
    "    \"FLOW_OPEN\", \"MODULE_LOAD\", \"PROCESS_CREATE\", \"PROCESS_OPEN\", \"PROCESS_TERMINATE\", \"REGISTRY_ADD\",\n",
    "    \"REGISTRY_EDIT\", \"REGISTRY_REMOVE\", \"SERVICE_CREATE\", \"SHELL_COMMAND\", \"TASK_CREATE\", \"TASK_DELETE\",\n",
    "    \"TASK_MODIFY\", \"TASK_START\", \"THREAD_CREATE\", \"THREAD_REMOTE_CREATE\", \"THREAD_TERMINATE\", \"USER_SESSION_GRANT\",\n",
    "    \"USER_SESSION_INTERACTIVE\",\"USER_SESSION_LOGIN\", \"USER_SESSION_LOGOUT\", \"USER_SESSION_REMOTE\", \"USER_SESSION_UNLOCK\"\n",
    "]\n",
    "NUM_BATCH = 1768\n",
    "NUM_GRAPH = NUM_BATCH * BATCH_SIZE\n",
    "NUM_WARMUP = 2 * max(NUM_NODE, NUM_GRAPH // BATCH_SIZE)\n",
    "\n",
    "EPOCH_NUM = 1000\n",
    "INIT_LR = 0.0003\n",
    "\n",
    "\n",
    "def get_weighted_adjacency_matrix(graph):\n",
    "    adj_spr = graph.adjacency_matrix(scipy_fmt=\"coo\")\n",
    "    edge_freq = graph.edata[\"action\"]\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    adj = torch.zeros(num_nodes, num_nodes)\n",
    "    for i, j, freq in zip(adj_spr.row, adj_spr.col, edge_freq):\n",
    "        adj[i, j] = freq\n",
    "    adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "\n",
    "def get_batch(path=\"data/train/\", index=0):\n",
    "    save_path = path + \"batch_\" + str(index) + \"/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        raise ValueError(\"The path does not exist\")\n",
    "    h = torch.load(save_path + \"h_\" + str(index) + \".pt\", weights_only=True)\n",
    "    pe = torch.load(save_path + \"pe_\" + str(index) + \".pt\", weights_only=True)\n",
    "    e = torch.load(save_path + \"e_\" + str(index) + \".pt\", weights_only=True)\n",
    "    return h, pe, e\n",
    "\n",
    "\n",
    "def sym_tensor(x):\n",
    "    x = x.permute(0, 3, 1, 2) # [bs, n, n, d]\n",
    "    triu = torch.triu(x,diagonal=1).transpose(3,2) # [bs, d, n, n]\n",
    "    mask = (triu.abs()>0).float()                  # [bs, d, n, n]\n",
    "    x =  x * (1 - mask ) + mask * triu             # [bs, d, n, n]\n",
    "    x = x.permute(0, 2, 3, 1) # [bs, n, n, d]\n",
    "    return x               # [bs, n, n, d]\n",
    "\n",
    "class Embed_G(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        self.Embed_h = nn.Embedding(params[\"num_type\"], d)\n",
    "        self.Embed_e = nn.Embedding(params[\"num_action\"], d)\n",
    "        self.Embed_pe = nn.Embedding(params[\"num_node\"], d)\n",
    "    \n",
    "    def forward(self, h, e, pe):\n",
    "        pe = self.Embed_pe(pe) # [bs, n, d]\n",
    "        h = self.Embed_h(h)\n",
    "        h = h + pe\n",
    "        e = self.Embed_e(e) # [bs, n, n, d]\n",
    "        e = e + pe.unsqueeze(1)\n",
    "        e = sym_tensor(e)\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class Attention_Layer(nn.Module):\n",
    "    def __init__(self, d, d_head, drop):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "        self.Drop_Att = nn.Dropout(drop)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        Q = self.Q(h) # [bs, n, d_head]\n",
    "        K = self.K(h)\n",
    "        V = self.V(h)\n",
    "        Q = Q.unsqueeze(2)  # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1)  # [bs, 1, n, d_head]\n",
    "        E = self.E(e)       # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(h).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(h).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = E + Ni + Nj              # [bs, n, n, d_head]\n",
    "        Att = (Q * e * K).sum(dim=-1) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1)             # [bs, n, n]\n",
    "        Att = self.Drop_Att(Att)\n",
    "        h = Att @ V # [bs, n, d_head]\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class MAH_Layer(nn.Module):\n",
    "    def __init__(self, d, head_num, drop):\n",
    "        super().__init__()\n",
    "        d_head = d // head_num\n",
    "        self.heads = nn.ModuleList([Attention_Layer(d, d_head, drop) for _ in range(head_num)])\n",
    "        self.WO_h = nn.Linear(d, d)\n",
    "        self.WO_e = nn.Linear(d, d)\n",
    "        self.Drop_h = nn.Dropout(drop)\n",
    "        self.Drop_e = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        h_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            h_mha, e_mha = head(h, e)\n",
    "            h_MHA.append(h_mha)\n",
    "            e_MHA.append(e_mha)\n",
    "        h = self.Drop_h(self.WO_h(torch.cat(h_MHA, dim=2)))\n",
    "        e = self.Drop_e(self.WO_e(torch.cat(e_MHA, dim=3)))\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class GT_Layer(nn.Module):\n",
    "    def __init__(self, d, num_head, drop):\n",
    "        super().__init__()\n",
    "        self.Norm_h_1 = nn.LayerNorm(d)\n",
    "        self.Norm_e_1 = nn.LayerNorm(d)\n",
    "        self.MHA = MAH_Layer(d, num_head, drop)\n",
    "        self.Norm_h_2 = nn.LayerNorm(d)\n",
    "        self.Norm_e_2 = nn.LayerNorm(d)\n",
    "        self.MLP_h = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.MLP_e = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.Drop_h = nn.Dropout(drop)\n",
    "        self.Drop_e = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        # h: [bs, n, d]; e: [bs, n, n, d]\n",
    "        h = self.Norm_h_1(h)\n",
    "        e = self.Norm_e_1(e)\n",
    "        h_MHA, e_MHA = self.MHA(h, e)\n",
    "        h = h + h_MHA\n",
    "        h = h + self.MLP_h(self.Norm_h_2(h))\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLP_e(self.Norm_e_2(e))\n",
    "        h = self.Drop_h(h)\n",
    "        e = self.Drop_e(e)\n",
    "        return h, e\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        d = params[\"d\"]\n",
    "        \n",
    "        # Graph Embedding\n",
    "        self.num_node = params[\"num_node\"]\n",
    "        self.Embed_he = Embed_G(params)\n",
    "        self.Embed_pe = nn.Embedding(params[\"num_node\"], d)\n",
    "\n",
    "        # GT Layers\n",
    "        num_enc_layer = params[\"num_enc_layer\"]\n",
    "        num_dec_layer = params[\"num_dec_layer\"]\n",
    "        num_head = params[\"num_head\"]\n",
    "        drop = params[\"drop\"]\n",
    "        self.Enc_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_enc_layer)])\n",
    "        self.Dec_Layers = nn.ModuleList([GT_Layer(d, num_head, drop) for _ in range(num_dec_layer)])\n",
    "\n",
    "        # Encoder\n",
    "        dz = params[\"dz\"]\n",
    "        self.LN_q_mu = nn.Linear(d, dz)\n",
    "        self.LN_q_logvar = nn.Linear(d, dz)\n",
    "\n",
    "        # Decoder\n",
    "        self.LN_p = nn.Linear(dz, d)\n",
    "\n",
    "        # Output Layer\n",
    "        self.Norm_Out_h = nn.LayerNorm(d)\n",
    "        self.Norm_Out_e = nn.LayerNorm(d)\n",
    "        self.LN_h = nn.Linear(d, params[\"num_type\"])\n",
    "        self.LN_e = nn.Linear(d, params[\"num_action\"])\n",
    "    \n",
    "    def forward(self, h, e, pe, num_node=None):\n",
    "        if num_node is None:\n",
    "            num_node = self.num_node\n",
    "\n",
    "        # Embedding\n",
    "        h, e = self.Embed_he(h, e, pe)\n",
    "        n = h.size(1)\n",
    "        pe = self.Embed_pe(pe)\n",
    "        # Encoder\n",
    "        for Enc_Layer in self.Enc_Layers:\n",
    "            h, e = Enc_Layer(h, e)\n",
    "            e = sym_tensor(e)\n",
    "        graph_token = h.mean(dim=1)\n",
    "        q_mu = self.LN_q_mu(graph_token)\n",
    "        q_logvar = self.LN_q_logvar(graph_token)\n",
    "        q_std = torch.exp(q_logvar / 2)\n",
    "        eps = torch.randn_like(q_std)\n",
    "        z = q_mu + eps * q_std # [bs, dz]\n",
    "        n = h.size(1)\n",
    "\n",
    "        # Decoder\n",
    "        z = self.LN_p(z) # [bs, d]\n",
    "        h = z.unsqueeze(1).repeat(1, n, 1) # [bs, n, d]\n",
    "        h = h + pe\n",
    "        e = z.unsqueeze(1).unsqueeze(1).repeat(1, n, n, 1) # [bs, n, n, d]\n",
    "        e = e + pe.unsqueeze(1) + pe.unsqueeze(2)\n",
    "        e = sym_tensor(e)\n",
    "        for Dec_Layer in self.Dec_Layers:\n",
    "            h, e = Dec_Layer(h, e)\n",
    "            e = sym_tensor(e)\n",
    "        h = self.Norm_Out_h(h)\n",
    "        e = self.Norm_Out_e(e)\n",
    "        h = self.LN_h(h)\n",
    "        e = self.LN_e(e)\n",
    "        return h, e, q_mu, q_logvar\n",
    "    \n",
    "\n",
    "def train_vae(net_params, load_save=True, model_path=\"model/\"):\n",
    "    torch.random.manual_seed(0)\n",
    "    vae = VAE(net_params).to(DEVICE)\n",
    "    if load_save:\n",
    "        if os.path.exists(model_path + \"vae.pt\"):\n",
    "            vae.load_state_dict(torch.load(model_path + \"vae.pt\", weights_only=True))\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    init_lr = INIT_LR\n",
    "    optimizer = torch.optim.AdamW(vae.parameters(), lr=init_lr)\n",
    "    scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/NUM_WARMUP, 1.0))\n",
    "    scheduler_tracker = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1)\n",
    "\n",
    "    num_epoch = EPOCH_NUM\n",
    "    num_warmup_batch = 0\n",
    "\n",
    "    train_loss_drop_patience = 5\n",
    "    loss_dropping = True\n",
    "    train_loss_drop_cnt = 0\n",
    "    previous_best_loss = float(\"inf\")\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        num_batch = 0\n",
    "\n",
    "        vae.train()\n",
    "\n",
    "        for i in range(NUM_BATCH):\n",
    "            h, pe, e = get_batch(index=i)\n",
    "            h = h.to(DEVICE)\n",
    "            pe = pe.to(DEVICE)\n",
    "            e = e.to(DEVICE)\n",
    "            \n",
    "            pred_h, pred_e, q_mu, q_logvar = vae(h, e, pe)\n",
    "            loss_data = torch.nn.CrossEntropyLoss()(pred_h.view(BATCH_SIZE * NUM_NODE, len(TYPES)), h.view(BATCH_SIZE * NUM_NODE)) \n",
    "            loss_data += torch.nn.CrossEntropyLoss()(pred_e.view(BATCH_SIZE * NUM_NODE * NUM_NODE, len(ACTIONS)), e.view(BATCH_SIZE * NUM_NODE * NUM_NODE))\n",
    "            loss_kl = -0.5 * torch.sum(1 + q_logvar - q_mu.pow(2) - q_logvar.exp())\n",
    "            loss = 2.5 * loss_data + loss_kl\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "\n",
    "            if num_batch < NUM_WARMUP:\n",
    "                scheduler_warmup.step()\n",
    "            num_batch += 1\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "            num_batch += 1\n",
    "\n",
    "            del h, pe, e, pred_h, pred_e, q_mu, q_logvar, loss_data, loss_kl, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        mean_loss = running_loss / num_batch\n",
    "        if num_warmup_batch >= NUM_WARMUP:\n",
    "            scheduler_tracker.step(mean_loss)\n",
    "        elapsed = (time.time() - start) / 60\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch}  Loss: {mean_loss:.6f}  lr: {optimizer.param_groups[0]['lr']:.6f}  Time: {elapsed:.2f} mins\")\n",
    "\n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if mean_loss < previous_best_loss:\n",
    "            previous_best_loss = mean_loss\n",
    "            train_loss_drop_cnt = 0\n",
    "            loss_dropping = True\n",
    "        else:\n",
    "            train_loss_drop_cnt += 1\n",
    "            loss_dropping = False\n",
    "            if train_loss_drop_cnt >= train_loss_drop_patience:\n",
    "                print(\"Early stopping since loss is not dropping\")\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 and loss_dropping:\n",
    "            torch.save(vae.state_dict(), model_path + \"vae.pt\")\n",
    "    \n",
    "    torch.save(vae.state_dict(), model_path + \"vae.pt\")\n",
    "    print(f\"Finished the training of VAE, with the best loss {previous_best_loss:.6f}, and the total time {elapsed:.2f} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed hyperparameter setting of the VAE model is shown in the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_enc_layer\": 4,\n",
    "    \"num_dec_layer\": 4,\n",
    "    \"num_head\": 8,\n",
    "    \"drop\": 0,\n",
    "    \"d\": 16 * 8,\n",
    "    \"dz\": 32\n",
    "}\n",
    "\n",
    "train_vae(vae_net_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process and Early Stopping\n",
    "\n",
    "Since the training dataset is large, the training process took much shorter time than expected. The training process for VAE model early stopped after 8 epochs, as the learning rate decayed less than the set threshold. The training process for DDPM model was manually stopped after 25 epochs, as the loss value had decreased to a very low level and did not change significantly. The last trainning loss value for DDPM model was around 0.003, and for VAE model was around 2.14.\n",
    "\n",
    "> The seperated training code Python files, example batches of graph data in tensor format, and the trained model parameters are available in the GitHub repository [here](https://github.com/Klasnov/graph_abnormal_detect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Results\n",
    "\n",
    "In this section, we will evaluate the trained DDPM and VAE models on the normal and abnormal testing log data. The analysis here is based on the assumption that all data points following the Gaussian distribution, which is also consistent with the Law of Large Numbers. Based on the reconstruction error of normal training data, we leveraged its mean $\\mu$ and standard deviation $\\sigma$ to calculate the possibility density function $f(x)=\\frac{1}{\\sqrt{2\\pi{\\sigma}^2}}e^{-\\frac{(x-\\mu)^2}{2{\\sigma}^2}}$ of the unseen data point $x$ being normal or abnormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subset of Normal Training Data\n",
    "\n",
    "A random subset of the normal training data is selected to calculate the mean and standard deviation of the reconstruction error. The subset contains 10% of the normal training data, which is consisted of 177 training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MAX_BATCH = 1768\n",
    "NUM_BATCH = 177\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "rand_indices = torch.randperm(MAX_BATCH)\n",
    "pick_indices = rand_indices[:NUM_BATCH]\n",
    "pick_indices = pick_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Standard Deviation Calculation for DDPM\n",
    "\n",
    "The mean and standard deviation of the reconstruction error for the DDPM model are calculated based on the random subset of the normal training data.\n",
    "\n",
    "From the excuted results, the mean and standard deviation of the reconstruction error for the DDPM model are calculated as $\\mu_{\\text{DM}}=0.0166$ and $\\sigma_{\\text{DM}}=0.0120$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 0.016599, Std Loss: 0.012046\n"
     ]
    }
   ],
   "source": [
    "beta_1 = 0.0001\n",
    "beta_t = 0.1\n",
    "num_t = 200\n",
    "\n",
    "ddpm_net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_gt_layer\": 6,\n",
    "    \"num_head\": 4,\n",
    "    \"d\": 32 * 4,\n",
    "    \"num_t\": num_t,\n",
    "    \"drop\": 0,\n",
    "    \"device\": DEVICE\n",
    "}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ddpm = DDPM(num_t, beta_1, beta_t, ddpm_net_params).to(DEVICE)\n",
    "ddpm.load_state_dict(torch.load(\"model/ddpm/ddpm.pt\", weights_only=True))\n",
    "\n",
    "ddpm.eval()\n",
    "loss_list = []\n",
    "for i in range(NUM_BATCH):\n",
    "    h, pe, e = get_batch(index=pick_indices[i])\n",
    "    h = h.to(DEVICE)\n",
    "    pe = pe.to(DEVICE)\n",
    "    e = e.to(DEVICE)\n",
    "    batch_sample_t = torch.randint(0, num_t, (BATCH_SIZE,)).long().to(DEVICE)\n",
    "    batch_noise_h_t = torch.randn(BATCH_SIZE, NUM_NODE, len(TYPES)).to(DEVICE)\n",
    "    batch_noise_e_t = torch.randn(BATCH_SIZE, NUM_NODE, NUM_NODE, len(ACTIONS)).to(DEVICE)\n",
    "    batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
    "\n",
    "    h_t, e_t = ddpm(h, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t)\n",
    "    noise_pred_h_t, noise_pred_e_t = ddpm.backward(h_t, e_t, pe, batch_sample_t)\n",
    "\n",
    "    loss = torch.nn.MSELoss()(noise_pred_h_t, batch_noise_h_t) + torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
    "    loss_list.append(loss.detach().item())\n",
    "\n",
    "    del h, pe, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t, h_t, e_t, noise_pred_h_t, noise_pred_e_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "loss = torch.tensor(loss_list)\n",
    "ddpm_mean = loss.mean().item()\n",
    "ddpm_std = loss.std().item()\n",
    "print(f\"Mean Loss: {ddpm_mean:.6f}, Std Loss: {ddpm_std:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Standard Deviation Calculation for VAE\n",
    "\n",
    "The mean and standard deviation of the reconstruction error for the VAE model are calculated based on the random subset of the normal training data.\n",
    "\n",
    "From the excuted results, the mean and standard deviation of the reconstruction error for the VAE model are calculated as $\\mu_{\\text{VA}}=4.8872$ and $\\sigma_{\\text{VA}}=2.1456$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 4.887179, Std Loss: 2.145647\n"
     ]
    }
   ],
   "source": [
    "vae_net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_enc_layer\": 4,\n",
    "    \"num_dec_layer\": 4,\n",
    "    \"num_head\": 8,\n",
    "    \"drop\": 0,\n",
    "    \"d\": 16 * 8,\n",
    "    \"dz\": 32\n",
    "}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "vae = vae = VAE(vae_net_params).to(DEVICE)\n",
    "vae.load_state_dict(torch.load(\"model/vae.pt\", weights_only=True))\n",
    "\n",
    "loss_list = []\n",
    "for i in range(NUM_BATCH):\n",
    "    h, pe, e = get_batch(index=pick_indices[i])\n",
    "    h = h.to(DEVICE)\n",
    "    pe = pe.to(DEVICE)\n",
    "    e = e.to(DEVICE)\n",
    "    pred_h, pred_e, q_mu, q_logvar = vae(h, e, pe)\n",
    "    loss_data = torch.nn.CrossEntropyLoss()(pred_h.view(BATCH_SIZE * NUM_NODE, len(TYPES)), h.view(BATCH_SIZE * NUM_NODE)) \n",
    "    loss_data += torch.nn.CrossEntropyLoss()(pred_e.view(BATCH_SIZE * NUM_NODE * NUM_NODE, len(ACTIONS)), e.view(BATCH_SIZE * NUM_NODE * NUM_NODE))\n",
    "    loss_kl = -0.5 * torch.sum(1 + q_logvar - q_mu.pow(2) - q_logvar.exp())\n",
    "    loss = 2.5 * loss_data + loss_kl\n",
    "    loss_list.append(loss.detach().item())\n",
    "\n",
    "    del h, pe, e, pred_h, pred_e, q_mu, q_logvar, loss_data, loss_kl, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "loss = torch.tensor(loss_list)\n",
    "vae_mean = loss.mean().item()\n",
    "vae_std = loss.std().item()\n",
    "print(f\"Mean Loss: {vae_mean:.6f}, Std Loss: {vae_std:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Processing\n",
    "\n",
    "We use 320 normal and 320 abnormal log graphs as the testing dataset. The reconstruction error of the models is calculated for each graph data point. The possibility of the data point being normal or abnormal is calculated based on the mean and standard deviation of the reconstruction error.\n",
    "\n",
    "The same as the training dataset processing, the edge features are converted from the COO format into the dense weighted adjacency matrix format. And the node features are reindexed with random values in the range of 0 to 99. Meanwhile, the graph data is also saved in the PyTorch tensor format for the model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "overview = pd.read_csv(\"data/actor_overview.csv\")\n",
    "overview = overview[(overview[\"data_type\"] == \"test\") & (overview[\"graphs\"] != 0)]\n",
    "\n",
    "NUM_GRAPH = 320\n",
    "\n",
    "def get_num_graphs(df, label, num_graph):\n",
    "    graphs_list = []\n",
    "    cnt = 0\n",
    "    flag = False\n",
    "    for actor_id in df[df[\"label\"] == label][\"actorID\"]:\n",
    "        chid_dirs_path = f\"data/test_graphs/{actor_id}/\"\n",
    "        if not os.path.exists(chid_dirs_path):\n",
    "            print(f\"Path {chid_dirs_path} does not exist\")\n",
    "            break\n",
    "        for file in os.listdir(chid_dirs_path):\n",
    "            if file.endswith(\".gz\"):\n",
    "                file_path = os.path.join(chid_dirs_path, file)\n",
    "                with gzip.open(file_path, \"rb\") as f:\n",
    "                    nx_graph = nx.read_gml(f)\n",
    "                    dgl_graph = nx_to_dgl(nx_graph)\n",
    "                    graphs_list.append(dgl_graph)\n",
    "                    cnt += 1\n",
    "                    if cnt == num_graph:\n",
    "                        flag = True\n",
    "                        break\n",
    "        if flag:\n",
    "            break\n",
    "    return graphs_list\n",
    "\n",
    "# Get normal and abnormal graphs\n",
    "normal_list = get_num_graphs(overview, 1, NUM_GRAPH)\n",
    "abnormal_list = get_num_graphs(overview, -1, NUM_GRAPH)\n",
    "\n",
    "# Save the graphs\n",
    "_, _, _ = batch_graph(normal_list, BATCH_SIZE, \"data/test/normal/\", 0, False)\n",
    "_, _, _ = batch_graph(abnormal_list, BATCH_SIZE, \"data/test/abnormal/\", 0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibility Density Results for DDPM\n",
    "\n",
    "Based on the $\\mu_{\\text{DM}}$ and $\\sigma_{\\text{DM}}$ calculated from the normal training data, the possibility density function of the testing data point being normal or abnormal is calculated for the DDPM model. The possibility density is calculated as $f_{\\text{DM}}(x)=\\frac{1}{\\sqrt{2\\pi{\\sigma_{\\text{DM}}}^2}}e^{-\\frac{(x-\\mu_{\\text{DM}})^2}{2{\\sigma_{\\text{DM}}}^2}}$, where $x$ is the reconstruction error of the testing data point. Here we show the mean value, standard deviation, median value, maximum value, and minimum value of the possibility density value for the normal and abnormal testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possibility Density Results for DDPM\n",
      "\n",
      "Normal Possibility Density Mean: 22.172440\n",
      "Normal Possibility Density Std: 3.967834\n",
      "Normal Possibility Density Median: 19.650785\n",
      "Normal Possibility Density Max: 29.338680\n",
      "Normal Possibility Density Min: 18.103816\n",
      "\n",
      "Abnormal Possibility Density Mean: 0.000169\n",
      "Abnormal Possibility Density Std: 0.000532\n",
      "Abnormal Possibility Density Median: 0.001682\n",
      "Abnormal Possibility Density Max: 0.000000\n",
      "Abnormal Possibility Density Min: 0.000000\n"
     ]
    }
   ],
   "source": [
    "TEST_BATCH = 10\n",
    "\n",
    "\n",
    "def gaussian_score(x, mean, std):\n",
    "    return torch.exp(-0.5 * ((x - mean) / std) ** 2) / (std * (2 * 3.1415) ** 0.5)\n",
    "\n",
    "\n",
    "beta_1 = 0.0001\n",
    "beta_t = 0.1\n",
    "num_t = 200\n",
    "\n",
    "ddpm_net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_gt_layer\": 6,\n",
    "    \"num_head\": 4,\n",
    "    \"d\": 32 * 4,\n",
    "    \"num_t\": num_t,\n",
    "    \"drop\": 0,\n",
    "    \"device\": DEVICE\n",
    "}\n",
    "\n",
    "del ddpm\n",
    "torch.cuda.empty_cache()\n",
    "ddpm = DDPM(num_t, beta_1, beta_t, ddpm_net_params).to(DEVICE)\n",
    "ddpm.load_state_dict(torch.load(\"model/ddpm.pt\", weights_only=True))\n",
    "print(\"Possibility Density Results for DDPM\")\n",
    "print()\n",
    "\n",
    "ddpm.eval()\n",
    "normal_losses = []\n",
    "for i in range(TEST_BATCH):\n",
    "    h, pe, e = get_batch(path=\"data/test/normal/\", index=i)\n",
    "    h = h.to(DEVICE)\n",
    "    pe = pe.to(DEVICE)\n",
    "    e = e.to(DEVICE)\n",
    "    batch_sample_t = torch.randint(0, num_t, (BATCH_SIZE,)).long().to(DEVICE)\n",
    "    batch_noise_h_t = torch.randn(BATCH_SIZE, NUM_NODE, len(TYPES)).to(DEVICE)\n",
    "    batch_noise_e_t = torch.randn(BATCH_SIZE, NUM_NODE, NUM_NODE, len(ACTIONS)).to(DEVICE)\n",
    "    batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
    "\n",
    "    h_t, e_t = ddpm(h, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t)\n",
    "    noise_pred_h_t, noise_pred_e_t = ddpm.backward(h_t, e_t, pe, batch_sample_t)\n",
    "\n",
    "    loss = torch.nn.MSELoss()(noise_pred_h_t, batch_noise_h_t) + torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
    "    normal_losses.append(loss.detach().item())\n",
    "\n",
    "    del h, pe, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t, h_t, e_t, noise_pred_h_t, noise_pred_e_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "normal_losses = torch.tensor(normal_losses)\n",
    "normal_possibility_density = gaussian_score(normal_losses, ddpm_mean, ddpm_std)\n",
    "torch.save(normal_possibility_density, \"results/ddpm_normal_possibility.pt\")\n",
    "print(f\"Normal Possibility Mean: {normal_possibility_density.mean().item():.6f}\")\n",
    "print(f\"Normal Possibility Std: {normal_possibility_density.std().item():.6f}\")\n",
    "print(f\"Normal Possibility Median: {normal_possibility_density.median().item():.6f}\")\n",
    "print(f\"Normal Possibility Max: {normal_possibility_density.max().item():.6f}\")\n",
    "print(f\"Normal Possibility Min: {normal_possibility_density.min().item():.6f}\")\n",
    "print()\n",
    "\n",
    "abnormal_losses = []\n",
    "for i in range(TEST_BATCH):\n",
    "    h, pe, e = get_batch(path=\"data/test/abnormal/\", index=i)\n",
    "    h = h.to(DEVICE)\n",
    "    pe = pe.to(DEVICE)\n",
    "    e = e.to(DEVICE)\n",
    "    batch_sample_t = torch.randint(0, num_t, (BATCH_SIZE,)).long().to(DEVICE)\n",
    "    batch_noise_h_t = torch.randn(BATCH_SIZE, NUM_NODE, len(TYPES)).to(DEVICE)\n",
    "    batch_noise_e_t = torch.randn(BATCH_SIZE, NUM_NODE, NUM_NODE, len(ACTIONS)).to(DEVICE)\n",
    "    batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
    "\n",
    "    h_t, e_t = ddpm(h, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t)\n",
    "    noise_pred_h_t, noise_pred_e_t = ddpm.backward(h_t, e_t, pe, batch_sample_t)\n",
    "\n",
    "    loss = torch.nn.MSELoss()(noise_pred_h_t, batch_noise_h_t) + torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
    "    abnormal_losses.append(loss.detach().item())\n",
    "\n",
    "    del h, pe, e, batch_sample_t, batch_noise_h_t, batch_noise_e_t, h_t, e_t, noise_pred_h_t, noise_pred_e_t\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "abnormal_losses = torch.tensor(abnormal_losses)\n",
    "abnormal_possibility_density = gaussian_score(abnormal_losses, ddpm_mean, ddpm_std)\n",
    "torch.save(abnormal_possibility_density, \"results/ddpm_abnormal_possibility.pt\")\n",
    "print(f\"Abnormal Possibility Mean: {abnormal_possibility_density.mean().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Std: {abnormal_possibility_density.std().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Median: {abnormal_possibility_density.median().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Max: {abnormal_possibility_density.max().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Min: {abnormal_possibility_density.min().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibility Density Results for VAE\n",
    "\n",
    "Based on the $\\mu_{\\text{VA}}$ and $\\sigma_{\\text{VA}}$ calculated from the normal training data, the possibility density function of the testing data point being normal or abnormal is calculated for the VAE model. The possibility density is calculated as $f_{\\text{VA}}(x)=\\frac{1}{\\sqrt{2\\pi{\\sigma_{\\text{VA}}}^2}}e^{-\\frac{(x-\\mu_{\\text{VA}})^2}{2{\\sigma_{\\text{VA}}}^2}}$, where $x$ is the reconstruction error of the testing data point. Here we show the mean value, standard deviation, median value, maximum value, and minimum value of the possibility density value for the normal and abnormal testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possibility Density for VAE\n",
      "\n",
      "Normal Possibility Density Mean: 0.177270\n",
      "Normal Possibility Density Std: 0.000674\n",
      "Normal Possibility Density Median: 0.177190\n",
      "Normal Possibility Density Max: 0.178772\n",
      "Normal Possibility Density Min: 0.176352\n",
      "      \n",
      "Abnormal Possibility Density Mean: 0.000000\n",
      "Abnormal Possibility Density Std: 0.000000\n",
      "Abnormal Possibility Density Median: 0.000000\n",
      "Abnormal Possibility Density Max: 0.000001\n",
      "Abnormal Possibility Density Min: 0.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_BATCH = 10\n",
    "\n",
    "\n",
    "def gaussian_score(x, mean, std):\n",
    "    return torch.exp(-0.5 * ((x - mean) / std) ** 2) / (std * (2 * 3.1415) ** 0.5)\n",
    "\n",
    "\n",
    "vae_net_params = {\n",
    "    \"num_type\": len(TYPES),\n",
    "    \"num_action\": len(ACTIONS),\n",
    "    \"num_node\": NUM_NODE,\n",
    "    \"num_enc_layer\": 4,\n",
    "    \"num_dec_layer\": 4,\n",
    "    \"num_head\": 8,\n",
    "    \"drop\": 0,\n",
    "    \"d\": 16 * 8,\n",
    "    \"dz\": 32\n",
    "}\n",
    "\n",
    "del vae\n",
    "torch.cuda.empty_cache()\n",
    "vae = vae = VAE(vae_net_params).to(DEVICE)\n",
    "vae.load_state_dict(torch.load(\"model/vae.pt\", weights_only=True))\n",
    "print(\"Possibility Density for VAE\")\n",
    "print()\n",
    "\n",
    "vae.eval()\n",
    "normal_losses = []\n",
    "for i in range(TEST_BATCH):\n",
    "    h, pe, e = get_batch(path=\"data/test/normal/\", index=i)\n",
    "    h = h.to(DEVICE)\n",
    "    pe = pe.to(DEVICE)\n",
    "    e = e.to(DEVICE)\n",
    "    pred_h, pred_e, q_mu, q_logvar = vae(h, e, pe)\n",
    "    loss_data = torch.nn.CrossEntropyLoss()(pred_h.view(BATCH_SIZE * NUM_NODE, len(TYPES)), h.view(BATCH_SIZE * NUM_NODE)) \n",
    "    loss_data += torch.nn.CrossEntropyLoss()(pred_e.view(BATCH_SIZE * NUM_NODE * NUM_NODE, len(ACTIONS)), e.view(BATCH_SIZE * NUM_NODE * NUM_NODE))\n",
    "    loss_kl = -0.5 * torch.sum(1 + q_logvar - q_mu.pow(2) - q_logvar.exp())\n",
    "    loss = 2.5 * loss_data + loss_kl\n",
    "    normal_losses.append(loss.detach().item())\n",
    "\n",
    "    del h, pe, e, pred_h, pred_e, q_mu, q_logvar, loss_data, loss_kl, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "normal_losses = torch.tensor(normal_losses)\n",
    "normal_possibility_density = gaussian_score(normal_losses, vae_mean, vae_std)\n",
    "torch.save(normal_possibility_density, \"results/vae_normal_pdf.pt\")\n",
    "print(f\"Normal Possibility Density Mean: {normal_possibility_density.mean().item():.6f}\")\n",
    "print(f\"Normal Possibility Density Std: {normal_possibility_density.std().item():.6f}\")\n",
    "print(f\"Normal Possibility Density Median: {normal_possibility_density.median().item():.6f}\")\n",
    "print(f\"Normal Possibility Density Max: {normal_possibility_density.max().item():.6f}\")\n",
    "print(f\"Normal Possibility Density Min: {normal_possibility_density.min().item():.6f}\")\n",
    "print()\n",
    "\n",
    "abnormal_losses = []\n",
    "for i in range(TEST_BATCH):\n",
    "    h, pe, e = get_batch(path=\"data/test/abnormal/\", index=i)\n",
    "    h = h.to(DEVICE)\n",
    "    pe = pe.to(DEVICE)\n",
    "    e = e.to(DEVICE)\n",
    "    pred_h, pred_e, q_mu, q_logvar = vae(h, e, pe)\n",
    "    loss_data = torch.nn.CrossEntropyLoss()(pred_h.view(BATCH_SIZE * NUM_NODE, len(TYPES)), h.view(BATCH_SIZE * NUM_NODE)) \n",
    "    loss_data += torch.nn.CrossEntropyLoss()(pred_e.view(BATCH_SIZE * NUM_NODE * NUM_NODE, len(ACTIONS)), e.view(BATCH_SIZE * NUM_NODE * NUM_NODE))\n",
    "    loss_kl = -0.5 * torch.sum(1 + q_logvar - q_mu.pow(2) - q_logvar.exp())\n",
    "    loss = 2.5 * loss_data + loss_kl\n",
    "    abnormal_losses.append(loss.detach().item())\n",
    "\n",
    "    del h, pe, e, pred_h, pred_e, q_mu, q_logvar, loss_data, loss_kl, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "abnormal_losses = torch.tensor(abnormal_losses)\n",
    "abnormal_possibility_density = gaussian_score(abnormal_losses, vae_mean, vae_std)\n",
    "torch.save(abnormal_possibility_density, \"results/vae_abnormal_pdf.pt\")\n",
    "print(f\"Abnormal Possibility Density Mean: {abnormal_possibility_density.mean().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Density Std: {abnormal_possibility_density.std().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Density Median: {abnormal_possibility_density.median().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Density Max: {abnormal_possibility_density.max().item():.6f}\")\n",
    "print(f\"Abnormal Possibility Density Min: {abnormal_possibility_density.min().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibility Density Analysis\n",
    "\n",
    "The performance of the DDPM and VAE models are concluded in the following table.\n",
    "\n",
    "|                    | DDPM PDF of Noramal Data | DDPM PDF of Abnormal Data | AVE PDF of Noramal Data | AVE PDF of Abnoramal Data |\n",
    "| :----------------: | :----------------------: | :-----------------------: | :---------------------: | :-----------------------: |\n",
    "|     Mean Value     |        22.172440         |         0.000169          |        0.177270         |         0.000000          |\n",
    "| Standard Deviation |         3.967834         |         0.000532          |        0.000674         |         0.000000          |\n",
    "|    Median Value    |        19.650785         |         0.001682          |        0.177190         |         0.000000          |\n",
    "|      Maximum       |        29.338680         |         0.000000          |        0.178772         |         0.000001          |\n",
    "|      Minimum       |        18.103816         |         0.000000          |        0.176352         |         0.000000          |\n",
    "\n",
    "By utilizing the possibility density function (PDF) of Gaussian distribution, both the DDPM and VAE models are able to detect the abnormal data points in a high accuracy. The score possibility of the normal testing data is much higher than the abnormal testing data across the mean, median, maximum, and minimum values all these statistical metrics. However, the PDF results of the DDPM model are much more distinguishable than the VAE model, as the mean and median values of the normal testing data are much higher than the abnormal testing data. We show the score possibility density and the performance difference in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "ddpm_norm = torch.load(\"results/ddpm_normal_pdf.pt\", weights_only=True)\n",
    "ddpm_abnorm = torch.load(\"results/ddpm_abnormal_pdf.pt\", weights_only=True)\n",
    "vae_norm = torch.load(\"results/vae_normal_pdf.pt\", weights_only=True)\n",
    "vae_abnorm = torch.load(\"results/vae_abnormal_pdf.pt\", weights_only=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot([ddpm_norm, ddpm_abnorm])\n",
    "plt.title(\"Possibility Density for DDPM\")\n",
    "plt.xticks([1, 2], [\"Normal Data\", \"Abnormal Data\"])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([vae_norm, vae_abnorm])\n",
    "plt.title(\"Possibility Density for VAE\")\n",
    "plt.xticks([1, 2], [\"Normal Data\", \"Abnormal Data\"])\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we can conclude that the DDPM model is more effective in detecting the abnormal data points in the log dataset, compared to the VAE model. Nevertheless, both models finished the task with high accuracy, as the normal data points are much more likely to be normal than the abnormal data points. The data analysis results are consistent with the model characteristics, as the DDPM model is more capable of capturing the underlying data distribution of the input data.\n",
    "\n",
    "We also want to mention that even though the results of our graph-based generation models meet the expectation, the detection score method we adopted here is very simple and naive. Much more sophisticated methods can be applied to improve the detection performance, like the ordinary differential equation (ODE) based score calculation. Due to the time limitation of the course project, we did not implement such advanced methods in this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
